---
title: "A Brief Tutorial on NMFregress"
author: Gabriel Phelan & Dave Campbell
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteEngine{knitr::knitr}
  %\VignetteIndexEntry{A Brief Tutorial on NMFregress}
  %\usepackage[UTF-8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document will serve as a brief introduction to the `NMFregress` package, which smashes together a host of ideas surrounding non-negative matrix factorization (NMF) and non-parametric inference. The point is to solve some problems that are typically addressed using complicated probabilistic graphical models. There's nothing wrong with graphical models; in fact, they're quite cool! The issue is that fitting them can be a major pain; entire careers have been built around inference algorithms for graphical models. Despite the rapid progress in this field -- think HMC and its implementation in the wonderful Stan package -- certain model classes have yet to be adorned with inference schemes whose speed, ease of use, and statistical accuracy are fully convincing. For this reason we take a different tack. The basic idea is to slap permutation tests and the boostrap on top of some recent developments in NMF research. Motivation is provided by topic models in which there is some sort of discrete covariate over the documents -- we might partition news stories by their source or a collection of books by author, for instance. This is the setting we'll stick to in what follows, but know that `NMFregress` can be used for good old NMF, whatever your application domain might be. The package's langauge is unabashedly slanted towards topic models, but you can simply ignore that and get on with factoring matrices if you wish; for example, a "term-document matrix" could really be any non-negative data matrix. Please refer to [link to thesis when available] for further details on the motivation and underlying mathematics, as well as references to the literature. 

## 1. Building a Corpus

We want to do topic modelling, so we'll need some documents. Let's use Romeo and Juliet (RJ), freely available on Project Gutenberg: https://www.gutenberg.org/ebooks/1112. Lines in the play will be considered documents, and we'll partition them by the act in which they appear. Since acts are deliberately chosen to divide a play's plot into coherent segments, we should be able to detect statistical variation in Shakespeare's language as the play progresses. Included is a term-document matrix (TDM) and a factor containing the play's acts. Let's load the library and the included data: 



```{r, message = FALSE}
library(NMFregress)
library(ggpubr) # for agganging the output of some plots.
```


## 2. Initializing an Object for NMF

The first step is to build an object that can be passed to the NMF solver. Rather than the solver requiring a slew of arguments, we instead create an object of class `nmf_input` which carries around the necesarry pieces, specified once and for all by the user. Most importantly, the user must create and pass a term-document matrix, which is a good ol' R matrix -- not a term-document matrix as created using some other topic modelling packages. You're on your own here -- we provide no functionality for creating a TDM or any of the other pre-processing that topic modelling usually entails. Also of note: we really mean TDM, and not DTM! Make sure the words are rows, otherwise everything will come out backwards! 
 
 
Once you've got your TDM, you need some covariate structure over the documents -- a partitioning, as we'll refer to it. These are the acts of RJ in our example. The first documents (which recall are lines in the play) belong to act one and so on.  The acts are encoded into a design matrix where _Act 1_ will be the intercept and subsequent acts are dummy encoded. 

```{r}
head(acts)
tail(acts)
```


The Term Document matrix for Romeo and Juliet has already been built with row labels corresponding to the vocabulary.  While not necessary, this is a convenient way of ensuring that you don't mix up rows.

```{r}
dim(Romeo_and_Juliet_tdm)

Romeo_and_Juliet_tdm[312:318, 184:193]
```

That's it! Now prepare for NMF by gathering the components to creates an object `my_input` of class `nmf_input` (this is basically just a named list) which is ready for factorization where it will use 25 topics.  

```{r}

my_input = create_input(Romeo_and_Juliet_tdm, 
                        vocab = rownames(Romeo_and_Juliet_tdm), 
                        covariates = acts, 
                        topics = 25)
my_input|> names()
```


## 3. Solving NMF 

Since `my_input` contains (nearly) all the information the solver needs, we now simply put 

```{r, results="hide"}
my_output = solve_nmf(my_input)
```

The resulting `my_output` is a class `nmf_output` object containing $\phi$ (the word-topic matrix), $\theta$ (the topic-document matrix), the anchor words, and some relevant information from `my_input` that the inference functions will need to do their thing:

```{r}
names(my_output)
class(my_output)
```

If you're just interested in the usual topic modelling pipeline, then you're done at this stage. Print the top words for each topic using the below code where `n=10` says we'd like 10 words printed per topic. Topics are labelled by their anchor word. 

```{r}
print_top_words(my_output, n = 10)
``` 



Now, a word on the optional argument in  `create_input(..., project = FALSE)`. This tells the solver that we do *not* want to perform random projections prior to solving NMF. Random projections send the data down to a random low-dimensional subspace, perform the linear algebra calculations there, and then map back up to the original space. Used appropriately, these can get provably close to the original solution while providing potentially large speed-ups. The TDM is small enough here where this isn't really an issue, and in fact such a projection may even be detrimental since "small" data is subject to higher distortion. But, for large matrices this is a really nice option; we'd specify something like this code (shown for example but won't be used elsewhere.)

```{r}
my_input2 = create_input(Romeo_and_Juliet_tdm,
                        vocab = rownames(Romeo_and_Juliet_tdm), 
                        covariates = acts, 
                        topics = 25)
my_output2 = solve_nmf(my_input2)
```

which tells the solver we'd like to project down to 200 dimensions to find the anchor words and likewise for solving the non-negative least squares problems that constitute convex NMF. It's up to you to get these dimensions right -- that is, ensuring that they are actually smaller than the input dimension (which is the number of columns in the TDM). For some context, we've solved NMF for matrices of size ~ 10,000 x 6,000 in a matter of minutes with very good results. Again, this is down to the data as well -- the more signal your TDM carries, the better these projections will work. 


## Quality of NMF fit

One may be interested in selecting the number of topics.  The right way to do this is based on whether or not the topics appropriately measure the hypothesis that you wish to test.  

Sometimes it is helpful to see how additional topics assist with the reconstruction error of the original TDM.   The `get_reconstruction_error` function calculates the Frobenius norm of reconstruction, $||X- \phi_t\theta_t||$, based on $t\in {1,2,...T}$ topics.  

```{r}
reconstruction_error = get_reconstruction_error(my_output, my_input)
reconstruction_error
```

## Normalization

Documents have different lengths and the user may see fit to handle this in one of a variety of ways depending on their goals and the domain of interest.  As with all pre-processing, choices have downstream impacts and results are conditional on these choices.  Normalization is the process of scaling the document decomposition in some way so as to avoid mundane inference that is to be interpreted as covariate X is associated with longer documents.  The preferred normalization method is to normalize the $\theta$ matrix so that the topic weights for a document sum to 1.  Normalizing $\theta$ provides the interpretation as the probability of topics for a document.  The bounded elements of $\theta \in [0,1]$, imply that a beta regression is appropriate.  However since topic allocations depend on the across-topic-sums for a document, the scaling will depend on the number of topics.  Inference for this model targets relative effect sizes conditional on the number of topics.  Topics are nearly orthogonal so typically changing the number of topics has little practical impact



## 4. Inferece I. OLS and bootstrap

The interpretation of performing regression on something akin to a probabilistic topic model implies that OLS will probably break the Gauss Markov theorem conditions.  However it is reasonable to use under some conditions.  Linear models are appropriate here if we set act as a factor and we use bootstrap for interval estimates.

 Recall that these are the play's acts. Basically, we're interested in the relationship between topics and groups. For this we run:

```{r, message = FALSE, warning=FALSE, results = FALSE }
OLS_bootstrap = boot_reg(my_output, samples = 1000, model = "OLS")

# Test the input of a subset of topics
```
A formula could be specified.  By default a formula is derived from all the columns of the covariate matrix.  Note that an intercept must be provided  as a column of the covariate matrix if you go this default route.  Here Act 1 is the intercept, with all other acts being interpreted as deviations thereof.

What we get is the bootstrap samples for the regression model of $text_i = \beta_0+ \sum_{act = 1}^4 X_{i,act} \beta_{act} + error_i$.

Let's extract and plot the effect sizes for the regression coefficients for the topics of **love** and **night**.
Note that we ran a model where act 1 was the baseline, so if all other effects are positive or negative relative to the baseline.  I am rounding the error bars, for viewing convenience. 


```{r, message = FALSE}
# plot the raw OLS coefficients:

ols_boot_juliet = brett_plot(OLS_bootstrap, "juliet") 
ols_boot_dead   = brett_plot(OLS_bootstrap, "dead")
ggpubr::ggarrange(ols_boot_juliet, ols_boot_dead, 
          ncol = 1, nrow = 2)

# or plot the model fit by including new data at which to evaluate the OLS model:
newdata = my_output$covariates|> unique()
rownames(newdata) = c("act1","act2","act3","act4","act5")

ols_fit_juliet = brett_plot(OLS_bootstrap, "juliet", newdata) 
ols_fit_dead   = brett_plot(OLS_bootstrap, "dead", newdata)
ggpubr::ggarrange(ols_fit_juliet, ols_fit_dead, 
          ncol = 1, nrow = 2)

create_error_bars(OLS_bootstrap, topic = c("dead"))|>
    dplyr::mutate(
      dplyr::across(where(is.numeric), ~round(.,digits =4))
      )

```

## 5. Inference II.  Beta regression and Asymptotic inference.

Beta regression is useful when the data is bounded (0,1) and we wish to model covariates which are factors of continuous.  Here we just have **acts** to model, so for the purposes of exposition let's assume that the topic of interest, **dead** may be building up as the play moves along.

Note that Beta regression is much slower than OLS.  We could still use bootstrap, but be prepared to wait orders of magnitude longer. Instead we set **samples=1** and **return_just_coefs=FALSE**.  This runs beta regression from the **betareg** package on each selected topic.  The entire output of the **betareg::betareg** is returned for each topic selected.  There is substantial flexibility for setting up models for the mean and precision.  Here by default we use the same model for each based on the full set of coefficients and the default logit link for the mean and a log link for precision.

First we swap out the covariates from the setup so that we run the model of interest. Then we select the topics of interest.  Here we will use:
\{"night","love","juliet", "death", "dead"\}.

```{r, message = FALSE, warning=FALSE, results = FALSE}

# just the model without bootstrap
beta_asymptotic = get_regression_coefs(my_output, 
                           model = "BETA", 
                           return_just_coefs = FALSE,
                           topics = c("night","love","juliet", "death", "dead"))

# to do full bootstrap:
beta_boot = boot_reg(my_output, 
                           samples = 100, model = "BETA", 
                           return_just_coefs = TRUE,
                           topics = c("night","love","juliet", "death", "dead"))


beta_asymptotic_juliet = brett_plot(beta_asymptotic, "juliet")
beta_asymptotic_dead   = brett_plot(beta_asymptotic, "dead")
ggpubr::ggarrange(beta_asymptotic_juliet, beta_asymptotic_dead, 
          ncol = 1, nrow = 2)
# or include new observations for the model:
newdata = my_output$covariates|> unique()
rownames(newdata) = c("act1","act2","act3","act4","act5")

beta_asymptotic_juliet_newdata = brett_plot(beta_asymptotic, "juliet", newdata = newdata)
beta_asymptotic_dead_newdata   = brett_plot(beta_asymptotic, "dead"  , newdata = newdata)
ggpubr::ggarrange(beta_asymptotic_juliet_newdata, beta_asymptotic_dead_newdata, 
          ncol = 1, nrow = 2)


beta_boot_juliet = brett_plot(beta_boot, "juliet")
beta_boot_dead   = brett_plot(beta_boot, "dead")
ggpubr::ggarrange(beta_asymptotic_juliet_newdata, beta_boot_juliet,
          beta_asymptotic_dead_newdata, beta_boot_dead,
          ncol = 2, nrow = 2)

# or include new observations for the model:
beta_boot_juliet_newdata = brett_plot(beta_boot, "juliet",newdata)
beta_boot_dead_newdata   = brett_plot(beta_boot, "dead",newdata)

ggpubr::ggarrange(beta_asymptotic_juliet_newdata, beta_boot_juliet,beta_boot_juliet_newdata,
          beta_asymptotic_dead_newdata, beta_boot_dead,beta_boot_dead_newdata,
          ncol = 3, nrow = 2)
```



Model output a list of length = the number of samples.  Since we have **samples =1** then we need to extract the model output from the first element of the list.  Then we can call by topic and use the asymptotic results for the beta regression.


```{r}
beta_asymptotic$death|> summary()
```

### Note about width of beta regression intervals:

Note that when running beta regression on a small number of documents will likely include _long tails_.  Here it shows up as the .025 and .5 quantiles being close to zero, but the .975 quantile is close to 1.  With more documents, there is more information that will tighten the density around the point estimate.

## 6. Inference III.  Generalized Additive Model with Beta Regression

This comes from the **mgcv** package using  _beta_ regression.  We need a continuous covariate so we build a univariate vector with values taken as the increments for each line within each act.

```{r, message = FALSE, warning=FALSE, results = FALSE }
# Each document is a line from the play, 
# so let's define line number as the fraction of the way through the particular act.
#

my_output$covariates = matrix(acts_continuous, ncol = 1)
colnames(my_output$covariates) = "acts"
beta_gam = get_regression_coefs(output = my_output, 
                    model =  "GAM", 
                    return_just_coefs = FALSE,
                    topics = c("night","love","juliet", "death", "dead")
         )
brett_plot(beta_gam, 
          topic = "death", 
          newdata = data.frame(acts=1:5))


brett_plot(beta_gam, 
          topic = "juliet", 
          newdata = data.frame(acts=1:5))


```


## Finally

So `juliet`, `death`, are increasingly likely as the story moves along.

